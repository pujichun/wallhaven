å®ä¸ç›¸ç’ï¼Œä½œä¸ºä¸€ä¸ªå–œæ¬¢æ”¶é›†å£çº¸çš„boyï¼Œæˆ‘ç›¯ä¸Š[wallhaven](https://wallhaven.cc/)å¾ˆä¹…äº†

ä½ å¯èƒ½è§‰å¾—æˆ‘åœ¨è¯´è¿™ç§é£æ ¼çš„![](https://w.wallhaven.cc/full/8o/wallhaven-8ogk2o.jpg)

å®ä¸ç›¸ç’æˆ‘å…¶å®ä¸å–œæ¬¢ï¼Œæˆ‘å–œæ¬¢çš„æ˜¯è¿™ç§

![](https://w.wallhaven.cc/full/95/wallhaven-95qjpd.jpg)

å“ˆå“ˆå“ˆ~anywaysï¼

èµ·å› æ˜¯æˆ‘çš„ç”µè„‘ç”¨çš„æ˜¯åŠ¨æ€å£çº¸ï¼ˆå†™äº†ä¸ªè‡ªåŠ¨åˆ‡æ¢çš„shellè„šæœ¬ï¼‰ï¼Œè€Œä¸€ä¸ªæœˆå‰æˆ‘ç”µè„‘ä¸Šåªæœ‰ä¸‰å¼ å£çº¸ï¼Œå½“æˆ‘çœ‹åˆ°[wallhaven](https://wallhaven.cc/)çš„å£çº¸æ—¶æˆ‘ç¬¬ä¸€æƒ³æ³•æ˜¯æˆ‘è¯¥æ‹¥æœ‰å®ƒä»¬ï¼Œå¦‚æœä½ å–œæ¬¢å£çº¸å¯èƒ½ä¹Ÿå’Œæˆ‘ä¸€æ ·ğŸ˜ğŸ˜ğŸ˜

![](https://gitee.com/pujichun/BlogImage/raw/master/img/L3TTVIR7OIJCTFW%60I%5DWOKV4.jpg)

![](https://gitee.com/pujichun/BlogImage/raw/master/img/6XDNIDS9J3WCOS2_.jpg)

æƒ³è¦è¿™äº›å£çº¸ä¸€å¼ ä¸€å¼ çš„æ‰‹åŠ¨ä¸‹è½½å½“ç„¶æ˜¯è¡Œä¸é€šçš„ï¼Œéœ€è¦å€ŸåŠ©ä¸€äº›ç‰¹æ®Šæ‰‹æ®µ

![](https://gitee.com/pujichun/BlogImage/raw/master/img/_20210519194905.png)

**æ­£ç‰‡å¼€å§‹**

![æ•ˆæœæ¼”ç¤º](https://raw.githubusercontent.com/pujichun/img/main/wallhaven.gif)

## è§£æç½‘é¡µ

ç¬”è€…è¦æŠ“å–çš„å£çº¸åœ°å€ï¼šhttps://wallhaven.cc/search?q=like%3Arddgwm

è§‚å¯Ÿåˆ—è¡¨é¡µï¼Œå‘ç°å‘ä¸‹æ»šåŠ¨åï¼Œ`url`ä¸­çš„`page`å‚æ•°ä¼šæ”¹å˜

![](https://gitee.com/pujichun/BlogImage/raw/master/img/20210519195344.png)

ç„¶åç®€å•æµ‹è¯•ä¸€ä¸‹åçˆ¬è™«ï¼ˆæ‰‹æ®µè¿‡äºæš´èºå°±ä¸å±•ç¤ºäº†ï¼‰ï¼Œå‘ç°æ²¡æœ‰å‘çˆ¬è™«ï¼Œä½†å‡ºäºå°Šæ•¬è¿˜æ˜¯æºå¸¦ä¸Š`User-Agent`ï¼Œç®€å•å°è£…ä¸€ä¸‹åˆ—è¡¨é¡µçš„è¯·æ±‚ï¼Œä¸€å…±åªæœ‰8é¡µï¼Œå¦‚æœè¯»è€…æ¯”è¾ƒé—²ä¹Ÿå¯ä»¥è‡ªè¡Œç ´è§£ä¸€ä¸‹`total page`

```python
import requests
from requests.exceptions import RequestException


def download_html(url) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code == 200:
            return resp.content.decode("utf8")
        return ""
    except RequestException:
        return ""


def run():
    link_list = []
    for i in range(1, 9):
        url = f"https://wallhaven.cc/search?q=like%3Arddgwm&page={i}"
        html = download_html(url)

if __name__ == "__main__":
    run()
```

é‚£ä¹ˆæ¥ä¸‹æ¥å°±è¦è§£æhtmlè·å–åˆ°å›¾ç‰‡urläº†

å½“æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…·å®šä½åˆ°å›¾ç‰‡çš„æ—¶å€™å‘ç°`src`çš„å€¼æ˜¯å°å›¾çš„é“¾æ¥

![](https://gitee.com/pujichun/BlogImage/raw/master/img/QQ%E6%88%AA%E5%9B%BE20210519201606.png)

ç‚¹å‡»å°é¢è¿›å»ä¹‹åå‘ç°å›¾ç‰‡æ˜¯è¿™æ ·çš„

![](https://gitee.com/pujichun/BlogImage/raw/master/img/QQ%E6%88%AA%E5%9B%BE20210519200554.png)

å¾ˆæ˜¾ç„¶è¿™ä¸ªé¡µé¢æ˜¯ç”±å‰ç«¯ç¨‹åºæ¸²æŸ“å‡ºæ¥çš„ï¼Œè¿™ä¸ªé¡µé¢ä¸æ˜¯å›¾ç‰‡ï¼Œå› æ­¤éœ€è¦é¼ æ ‡å³é”®ç‚¹å‡»è¿™ä¸ªé¡µé¢ä¸­çš„å¤§å›¾ç‰‡ï¼Œé€‰ä¸­`åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€å›¾ç‰‡`

![](https://gitee.com/pujichun/BlogImage/raw/master/img/20210519201145.png)

å½“ç„¶ä¹Ÿå¯ä»¥ä»è¿™ä¸ªé¡µé¢ä¸­è§£æåˆ°å›¾ç‰‡é“¾æ¥ï¼Œä½†æ˜¯æ²¡æœ‰å¿…è¦

åœ¨æ–°æ‰“å¼€çš„çª—å£ä¸­å‘ç°urlå˜æˆäº†è¿™æ ·`https://w.wallhaven.cc/full/rd/wallhaven-rddgwm.jpg`ï¼Œè¿™ä¸ªé“¾æ¥å’Œåˆ—è¡¨é¡µä¸­`src`çš„å€¼å¾ˆåƒï¼Œç®€å•å†™ä¸ªå‡½æ•°è½¬æ¢ä¸€ä¸‹

```python
def image_link_process(image_link: str) -> str:
    # https://w.wallhaven.cc/full/rd/wallhaven-rddgwm.jpg
    # https://th.wallhaven.cc/small/rd/rddgwm.jpg
    if "small" in image_link:
        image_link = image_link.replace("th", "w")
        image_link = image_link.replace("small", "full")
        url_path = image_link.split("/")
        url_path[-1] = f"wallhaven-{url_path[-1]}"
        return "/".join(url_path)
    return ""
```

æ¬§å…‹ï¼è¿è¡Œä¹‹åå‘ç°èƒ½è½¬æ¢æˆåŠŸï¼Œé‚£ä¹ˆä¸‹ä¸€æ­¥å°±æ˜¯æå–åˆ°`src`çš„å€¼ï¼Œç¼–å†™ä¸€ä¸ªç®€å•çš„è„šæœ¬æµ‹è¯•ä¸€ä¸‹

```python
import requests
from lxml import etree


def test():
    url = "https://wallhaven.cc/search?q=like%3Arddgwm&page=1"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    resp = requests.get(url, headers=headers)
    html = resp.content.decode("utf8")
    print(html, end="\n\n\n")
    element = etree.HTML(html)
    image_links = element.xpath('//img[@alt="loading"]/@src')
    print(image_links)


if __name__ == '__main__':
    test()

```

å‘ç°ä¸å¯¹å•Š

![](http://img.doutula.com/production/uploads/image/2016/07/11/20160711198823_aAEDKy.jpg)

æŸ¥çœ‹è¾“å‡ºçš„htmlï¼Œå‘ç°å¯¹åº”æ ‡ç­¾æ²¡æœ‰`src`å±æ€§ï¼Œä½†æ˜¯æœ‰`data-src`å±æ€§ï¼Œå› æ­¤æå–`data-src`çš„å€¼ï¼Œå°†`xpath`è¡¨è¾¾å¼æ”¹ä¸º

```
//img[@alt="loading"]/@data-src
```

å†æ¬¡è¿è¡Œï¼è¿™ä¸‹æ¬§å…‹äº†ï¼

æ¥ä¸‹æ¥å°±æ˜¯é‡å¤´æˆäº†ï¼ä¸‹è½½å›¾ç‰‡ï¼

## ä¸‹è½½å›¾ç‰‡

é€šå¸¸åœ¨ç”¨`requests`è·å–ç½‘é¡µçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨`text`æˆ–è€…`content.decode()`çš„æ–¹æ³•å°†è¯·æ±‚è¿”å›çš„æ•´ä¸ª`body`è¯»å‡ºæ¥ï¼Œä½†æ˜¯å¯¹äº[wallhaven](https://wallhaven.cc/)ä¸Šåˆ†è¾¨ç‡æ™®ééƒ½å¾ˆé«˜çš„å›¾ç‰‡è€Œè¨€ï¼Œæˆ‘ä»¬åº”è¯¥åˆ†å—è¯»å–ã€‚

å› ä¸ºå›¾ç‰‡ä¼ è¾“çš„è¿‡ç¨‹æœ¬æ¥å°±æ˜¯ä¸€ä¸ªæµå¼ä¼ è¾“ï¼Œä½¿ç”¨`response.get(stream=True)`å°†æ¨è¿Ÿå“åº”å†…å®¹çš„ä¸‹è½½ï¼Œå½“æˆ‘ä»¬è¯»å–çš„æ—¶å€™æ‰ä¼šä¸‹è½½

å› ä¸ºè¦åˆ†å—è¯»å–ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ¯æ¬¡è¯»ä¸€å®šæ•°é‡çš„å­—èŠ‚çš„æ–‡ä»¶å‡ºæ¥ï¼Œæ‰€ä»¥è¦ä½¿ç”¨`response.iter_content(chunk)`çš„æ–¹å¼å»è¯»ï¼Œä¸€æ¬¡è¯»`chunk`ä¸ªå­—èŠ‚çš„å†…å®¹

```python
def download_image(image_link):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    resp = requests.get(image_link, headers=headers, stream=True)
    try:
        if resp.status_code == 200:
            with open(file=f'images/{image_link.split("/")[-1]}', mode="wb") as f:
                for chunk in resp.iter_content(chunk_size=1024):
                    f.write(chunk)
			print("ä¸‹è½½æˆåŠŸ")
        else:
            print("ä¸‹è½½å¤±è´¥")
    except:
        print("ä¸‹è½½å¤±è´¥")
```

## è¿›åº¦æ¡

æœ¬æ–‡ä½¿ç”¨çš„æ”¯æŒè¿›åº¦æ¡çš„ç¬¬ä¸‰æ–¹åº“ä¸º`rich`ï¼Œ`rich`åŠŸèƒ½ååˆ†å¼ºå¤§

å®‰è£…ååœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å°±èƒ½çœ‹åˆ°æ ·ä¾‹å±•ç¤º

```shell
python -m rich.progress
```

å› ä¸ºä»£ç ç®€å•æ‰€ä»¥å°±ä¸åºŸè¯äº†

```python
def download_image(image_link):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    resp = requests.get(image_link, headers=headers, stream=True)
    try:
        # è·å–å›¾ç‰‡æ€»å­—èŠ‚æ•°
        file_size = int(resp.headers['content-length'])
        if resp.status_code == 200:
            with open(file=f'images/{image_link.split("/")[-1]}', mode="wb") as f:
                # åˆ›å»ºå¹¶å¯åŠ¨Progressï¼ˆè¿›åº¦æ¡ï¼‰å¯¹è±¡
                with Progress() as progress:
                    # ç»™Progresså¯¹è±¡æ·»åŠ ä¸€ä¸ªä»»åŠ¡ï¼Œå¹¶æŒ‡å®šä»»åŠ¡å¤§å°
                    task1 = progress.add_task(f'[red]Downloading...{f.name}', total=file_size)
                    for chunk in resp.iter_content(chunk_size=1024):
                        f.write(chunk)
                        # æ›´æ–°è¿›åº¦æ¡
                        progress.update(task1, advance=1024)
        else:
            print("ä¸‹è½½å¤±è´¥")
    except:
        print("ä¸‹è½½å¤±è´¥")
```

é€šè¿‡`Progress`å¯¹è±¡å¯ä»¥åˆ›å»ºå¤šæ¡è¿›åº¦æ¡

## å®Œæ•´ç¨‹åºå¦‚ä¸‹

```python
import requests
from requests.exceptions import RequestException
from lxml import etree
from typing import List
from rich.progress import Progress


def download_html(url) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code == 200:
            return resp.content.decode("utf8")
        return ""
    except RequestException:
        return ""


def parse_image_link(html: str) -> List:
    element = etree.HTML(html)
    links = element.xpath('//img[@alt="loading"]/@data-src')
    return links


def image_link_process(image_link: str) -> str:
    # https://w.wallhaven.cc/full/v9/wallhaven-v9gk6l.jpg
    # https://th.wallhaven.cc/small/v9/v9gk6l.jpg
    if "small" in image_link:
        image_link = image_link.replace("th", "w")
        image_link = image_link.replace("small", "full")
        url_path = image_link.split("/")
        url_path[-1] = f"wallhaven-{url_path[-1]}"
        return "/".join(url_path)
    return ""


def download_image(image_link):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
    }
    resp = requests.get(image_link, headers=headers, stream=True)
    try:
        file_size = int(resp.headers['content-length'])
        if resp.status_code == 200:
            with open(file=f'images/{image_link.split("/")[-1]}', mode="wb") as f:
                with Progress() as progress:
                    task1 = progress.add_task(f'[red]Downloading...{f.name}', total=file_size)
                    for chunk in resp.iter_content(chunk_size=1024):
                        f.write(chunk)
                        progress.update(task1, advance=1024)
        else:
            print("ä¸‹è½½å¤±è´¥")
    except:
        print("ä¸‹è½½å¤±è´¥")


def run():
    link_list = []
    for i in range(1, 9):
        url = f"https://wallhaven.cc/search?q=like%3Arddgwm&page={i}"
        html = download_html(url)
        links = parse_image_link(html)
        link_list.extend(links)
    for image_link in link_list:
        image_link = image_link_process(image_link)
        download_image(image_link)


if __name__ == '__main__':
    run()
    
```

![](http://img.doutula.com/production/uploads/image/2020/03/28/20200328365125_DSQGkA.jpg)

è¿™å°±å®Œäº†ï¼Ÿå½“ç„¶æ²¡æœ‰ï¼Œå¯èƒ½å„ä½è¯»è€…å·²ç»æƒ³å¥½æ€ä¹ˆç”¨å¤šçº¿ç¨‹æˆ–è€…å¤šè¿›ç¨‹åŠ é€Ÿäº†ï¼Œä½†æ˜¯å•Šï¼å°±æ˜¯è¿™ä¸ªä½†æ˜¯ï¼å¤šçº¿ç¨‹å¤šæ²¡æ„æ€ï¼åç¨‹~~~å†²ï¼

![](http://img.doutula.com/production/uploads/image/2017/12/14/20171214258818_lMAnGx.jpg)

~~åç¨‹å†™æ³•å¾…æ›´æ–°~~

